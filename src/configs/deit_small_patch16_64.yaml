# Architecture 79.9%
arch: deit_small_patch16_64

# ===== Dataset ===== #
data_url: /home/mindspore//ms-deit/src/data/tiny-imagenet-200/
set: ImageNet
num_classes: 200
mix_up: 0.8
cutmix: 1.0
auto_augment: rand-m9-mstd0.5-inc1
interpolation: bicubic
re_prob: 0.25
re_mode: pixel
re_count: 1
mixup_prob: 1.0
switch_prob: 0.5
mixup_mode: batch
image_size: 64
crop_pct: 0.9


# ===== Learning Rate Policy ======== #
optimizer: adamw
base_lr: 0.0005
warmup_lr: 0.000001
min_lr: 0.000001
lr_scheduler: cosine_lr
warmup_length: 5

# ===== Network training config ===== #
amp_level: O2
beta: [ 0.9, 0.999 ]
clip_global_norm_value: 5.
is_dynamic_loss_scale: True
epochs: 300
cooldown_epochs: 10
label_smoothing: 0.1
weight_decay: 0.05
momentum: 0.9
batch_size: 128
drop_path_rate: 0.1

# ===== EMA ===== #
with_ema: False
ema_decay: 0.9999

# ===== Hardware setup ===== #
num_parallel_workers: 1
device_target: GPU